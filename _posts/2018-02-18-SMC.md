---
layout: post
title: Sequential Monte Carlo
tags: statistics method
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


## Sequential Monte Carlo

### Motivation
Considering dynamic linear models, if each state follows Gaussian distribution and transformation is linear, then it is positive to derive an explicit expression to compute the evolcing sequence of posterior distribution. The recursion is well known as Kalman Filter. It is widely used in Hidden Markov Model. From Bayesian aspect, the forward backward algorithm can be used to sample hidden states from their posterior distributions. It is same for the continuous Hidden Markov Model, but the transition matrix should be computed from the transition intensity matrix. 

However, real data is very complicated typically involving elements of non-Gaussian, high dimensionality and nonlinearity. It is intractable to derive the exact posterior distribution. Therefore, many approximations schemes, such as extended Kalman filter, Gaussian sum approximation and grid-based filters, have been proposed. Fisrt two methods usually give poor results and the griod- based filter is too computational expensive especially for hign dimensions.

The ensemble Kalman filter is a popular data-driven approximation schemes. Hte key idea is to use sampling data for each state to approximate the true distribution.

#Sequential Monte Carlo# methods are a set of simulation-based methods for posterior distirubtion computation. It is widely used because it is very flexible and easy to implement and parallelisable.

### State Space Model
$P(x_0)$

$P(x_t|x_{t-1})$ for $t \leq 1$

$P(y_t|x_t) for $t\leq 0$

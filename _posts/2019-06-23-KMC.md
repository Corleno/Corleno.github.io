---
layout: post
title: Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families
tag: machine learning
---

## Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families

### Introduction
* Pseudo-Marginal MCMC (PM-MCMC) is proposed when the posterior does not have an analytic expression but only can be estimated at any given point. (M.A. Beaumont 2003, C. Andrieu 2009)
* Approximate Bayesian Computation (ABC-MCMC) approximate the posterior via repeating simulation from a likelihood model. (P.Marjoram 2003, S.A. Sisson 2010)
* Adaptive Metropolis-Hastings (AMH) is based on learning the global scaling of the target from the history of the Markov Chain. (H. Haario 1999, C.Andrieu 2008)

### Background
* (Kernel) Adaptive Metropolis-Hastings:
  In the absence of $\triangledown \log \pi$, the choice of proposal distribution is a random walk, i.e. $Q(\cdot|x_t) = \mathcal{N}(\cdot|x_t, \Sigma_t)$. A popular choice of the scalling is $\Sigma_t \propto I$. AMH improves mixing by adaptively learning global coveriance structure of $\pi$ from the history of the Markov chain. When the local scaling does not match the global covariance of $\pi$, KAMU improves mixing by learing the target covariance in a RKHS.

* Hamiltonian Monte Carlo:
  It use deterministic, measure-preserving maps to generate efficient Markov transitions. It starts from the negative log target, referred to a the potential energy $U(q) = -\log \pi(q)$ with a kinetic energy $K(p)$. The joint distribution of $(p,q)$ is propotional to $\exp(-H(p,q))$, where $H(p,q) = K(p) + U(q)$ is called the Hamiltonian. Then $H(p,q)$ defines a Hamiltonian flow based on Hamiltonian operator
  \begin{equation} 
  \frac{\partial K}{\partial p}\frac{\partial}{\partial q} - \frac{\partial U}{\partial q}\frac{\partial}{\partial p} \label{Hamiltionian_opt}
  \end{equation}, which is a map $\phi_t^H: (p,q) \rightarrow (p^{\*}, q^{\*})$ such that $H(p^{\*}, q^{\*}) = H(p,q)$. In practice, the transition is usually unavailable and we need to resort to a approximations which is called leap-frog numerical solution. It discretise the continuous movement to $\triangle t$. However, this approximation introduce discretisation error and then a Metropolis acceptance procedure can be applied. 

* Kernel Induced Hamiltonian Dynamics: KMC replaces the potential energy in \eqref{Hamiltionian_opt} by a kernel induced surrogate computed from the history of the Markov chain. 

### Kernel Induced Hamiltonian Dynamics
* Infinite dimensional exponential family model
\begin{equation}
const \times \pi(x) \approx \exp(\<f, k(x,\cdot)\>_{\mathcal{H}} - A(f))
\end{equation}
where $\mathcal{H}$ is a RKHS of real valued functions on $\mathcal{X}$.
* Minimise the expected gradient mismatch between the model and the true target density through score matching approach (Aapo 2005), to avoid the intractable A(f).
* Use kernel surrogate $U_k = f$ to replace the true potential energy $U$, and discretisation error from the leap-frog is naturally corrected for in the Pseudo-Marginal Metroplolis step.
* Score matching on infinite demensional exponential families:

\begin{equation}
J(f) = \frac{1}{2}\int_\mathcal{X}\pi(x)\|\triangledown\log \tilde{\pi}(x; f) - \triangledown\log\pi(x)\|_2^2dx 
\end{equation}

with empirical version:

\begin{equation}
\hat{J}(f) = \frac{1}{n}\sum_{x\in\mathcal{D}}\sum_{\ell = 1}^d\left[\frac{\partial^2\log\tilde{\pi}(x;f)}{\partial x_\ell^2} + \frac{1}{2}\left(\frac{\partial\log\tilde{\pi}(x;f)}{\partial x_\ell}\right)^2\right]\,.
\end{equation}

* Infinite Dimensional Exponential Families Lite: $f(x) = \sum_{i = 1}^n \alpha_i k(z_i,x)$ where $\alpha\in \mathbb{n}$ are real valued parameters and $\{z_i\}_{i = 1}^n \subseteq \{x_i\}_{i = 1}^t$.
* Exponential Families in Finite Feature Spaces: $f(x) = \< \theta, \phi_x \>_{\mathcal{H}}$.
